# @package megatron_config.policy
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
context_parallel_size: 1
expert_model_parallel_size: 1
expert_tensor_parallel_size: null

# Settings for the Distributed Data Parallel (DDP) config
ddp_config:
  grad_reduce_in_fp32: true
  overlap_grad_reduce: false
  overlap_param_gather: false
  average_in_collective: true

# kwargs to override the HF model config
model_config_kwargs: {}

torch_profiler_config:
  enable: false
  ranks: []
  save_path: null

# kwargs to override the Megatron OptimizerConfig object
# any overlapping arguments with those in trainer.policy.optimizer_config will be overridden by the values here
optimizer_config_kwargs:
  overlap_cpu_optimizer_d2h_h2d: false
  use_precision_aware_optimizer: false
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0

# kwargs to override the Megatron TransformerConfig object
transformer_config_kwargs:
  # Recompute config - used for gradient/activation checkpointing
  # for details see: https://github.com/NVIDIA/Megatron-LM/blob/core_r0.13.0/megatron/core/transformer/transformer_config.py#L33
  # options: ["full", "selective", null]
  recompute_granularity: null

  recompute_modules: ["core_attn"]

  # options: ["uniform", "block", null]
  # 'uniform' divides the total number of transformer layers and checkpoints the input activation of each chunk
  # 'block' checkpoints the specified number of layers per pipeline stage at the specified granularity
  recompute_method: null

  # when recompute_method is uniform, recompute_num_layers is the number of transformer layers in each recompute unit
  # when recompute_method is block, recompute_num_layers is the number of transformer layers to recompute in each block
  # must be set to None when recompute_granularity is "selective"
  recompute_num_layers: null
