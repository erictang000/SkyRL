# @package megatron_config.policy
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
context_parallel_size: 1
expert_model_parallel_size: 1
expert_tensor_parallel_size: null

# Settings for the Distributed Data Parallel (DDP) config
ddp_config:
  grad_reduce_in_fp32: true
  overlap_grad_reduce: false
  overlap_param_gather: false
  average_in_collective: true

# kwargs to override the HF model config
model_config_kwargs: {}

torch_profiler_config:
  enable: false
  ranks: []
  save_path: null

# kwargs to override the Megatron OptimizerConfig object
# any overlapping arguments with those in trainer.policy.optimizer_config will be overridden by the values here
optimizer_config_kwargs:
  # set these all to true (and optimizer_offload_fraction=1.0) for optimizer cpu offloading
  overlap_cpu_optimizer_d2h_h2d: false
  use_precision_aware_optimizer: false
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0

# kwargs to override the Megatron TransformerConfig object
transformer_config_kwargs:
  # Recompute config - used for gradient/activation checkpointing
  # for details see: https://github.com/NVIDIA/Megatron-LM/blob/core_r0.13.0/megatron/core/transformer/transformer_config.py#L33
  # for the most aggresive memory savings, set recompute_granularity to "full", recompute_method to "uniform", and recompute_num_layers to 1
  recompute_granularity: null
  recompute_modules: ["core_attn"]
  recompute_method: null
  recompute_num_layers: null
